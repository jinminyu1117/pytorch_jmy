{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autograd: 自动求导机制\n",
    "\n",
    "PyTorch 所有神经网络的核心是 ``autograd`` 包。先简单介绍一下这个包，然后训练第一个简单的神经网络。\n",
    "\n",
    "``autograd``包为张量上的所有操作提供了自动求导。\n",
    "它是一个运行时定义的框架，意味着反向传播根据代码确定如何运行，并且每次迭代可以不同。\n",
    "\n",
    "\n",
    "### 张量 Tensor\n",
    "\n",
    "``torch.Tensor``是这个包的核心类。如果设置``.requires_grad`` 为``True``，会追踪所有对于该张量的操作。 \n",
    "完成计算后调用 ``.backward()``自动计算所有梯度，这个张量的所有梯度自动积累到``.grad``属性。\n",
    "\n",
    "要阻止张量跟踪历史记录，调用``.detach()``方法将其与计算历史记录分离，禁止跟踪将来的计算记录。\n",
    "\n",
    "为了防止跟踪历史记录（使用内存），将代码块包装在``with torch.no_grad()：``中。\n",
    "评估模型特别有用，因为模型可能有`requires_grad=True`的可训练参数，但不需要计算梯度。\n",
    "\n",
    "自动梯度计算中还有另外一个重要类``Function``.\n",
    "\n",
    "``Tensor``和``Function``互相连接生成一个非循环图，表示和存储完整的计算历史。\n",
    "每个张量都有一个``.grad_fn``属性，这个属性引用一个创建了``Tensor``的``Function``\n",
    "（除非用户手动创建这个张量，即这个张量的``grad_fn``是``None``）。\n",
    "\n",
    "如果需要计算导数，可以在``Tensor``上调用``.backward()``。 \n",
    "如果``Tensor``是一个标量（即包含一个元素数据），不需要为``backward()``指定任何参数。\n",
    "如果有更多元素，需要指定一个``gradient`` 参数匹配张量形状。\n",
    "\n",
    "其他文章可能会看到将Tensor包裹到Variable中提供自动梯度计算，Variable在0.41版中已经被标注为过期了。\n",
    "现在可以直接使用Tensor，官方文档在这里：(https://pytorch.org/docs/stable/autograd.html#variable-deprecated) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "创建一个张量并设置 requires_grad=True 追踪计算历史"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(2, 2, requires_grad=True)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对张量进行操作:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3., 3.],\n",
      "        [3., 3.]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "y = x + 2\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "结果``y``已经被计算出来，``grad_fn``已经被自动生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<AddBackward0 object at 0x12d478390>\n"
     ]
    }
   ],
   "source": [
    "print(y.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对y进行一个操作\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[27., 27.],\n",
      "        [27., 27.]], grad_fn=<MulBackward0>) tensor(27., grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "z = y * y * 3\n",
    "out = z.mean()\n",
    "print(z, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``.requires_grad_( ... )``可以改变现有张量``requires_grad``属性。\n",
    "如果没有指定，默认输入flag是 ``False``。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "None\n",
      "True\n",
      "<SumBackward0 object at 0x12d8f1d30>\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(2, 2)\n",
    "a = (a * 3) / (a - 1)\n",
    "print(a.requires_grad)\n",
    "print(a.grad_fn)\n",
    "\n",
    "a.requires_grad_(True)\n",
    "print(a.requires_grad)\n",
    "\n",
    "b = (a * a).sum()\n",
    "print(b.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 梯度 反向传播\n",
    "``out``是一个纯量（scalar），``out.backward()`` 等于``out.backward(torch.tensor(1))``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print gradients d(out)/dx\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.5000, 4.5000],\n",
      "        [4.5000, 4.5000]])\n"
     ]
    }
   ],
   "source": [
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``out``叫做*Tensor* “$o$”.得到 $o = \\frac{1}{4}\\sum_i z_i$,\n",
    "$z_i = 3(x_i+2)^2$ 和 $z_i\\bigr\\rvert_{x_i=1} = 27$.\n",
    "\n",
    "因此,\n",
    "$\\frac{\\partial o}{\\partial x_i} = \\frac{3}{2}(x_i+2)$, 则\n",
    "$\\frac{\\partial o}{\\partial x_i}\\bigr\\rvert_{x_i=1} = \\frac{9}{2} = 4.5$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果有向量值函数 $\\vec{y} = f(\\vec{x}))$ ，且 $\\vec{y}$ 关于 $\\vec{x}$ 的梯度是一个雅可比矩阵(Jacobian matrix)：\n",
    "\n",
    "$J = \\begin{pmatrix} \\frac{\\partial y_{1}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{1}}{\\partial x_{n}} \\\\ \\vdots & \\ddots & \\vdots \\\\ \\frac{\\partial y_{m}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{n}} \\end{pmatrix}$\n",
    "\n",
    "`torch.autograd`用来计算vector-Jacobian product。也就是说，给定任一向量 $v=(v_{1}\\;v_{2}\\;\\cdots\\;v_{m})^{T}$ ，计算 $v^{T}\\cdot J$。如果 $v$ 恰好是标量函数 $l=g(\\vec{y})$ 的梯度，也就是说 $v=(\\frac{\\partial l}{\\partial  y_{1}}\\;\\cdots\\;\\frac{\\partial l}{\\partial  y_{m}})^{T}$，那么根据链式法则，vector-Jacobian product 是 $\\vec{x}$ 对 $l$ 的梯度：\n",
    "\n",
    "$J^{T}\\cdot v = \\begin{pmatrix} \\frac{\\partial y_{1}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{1}} \\\\ \\vdots & \\ddots & \\vdots \\\\ \\frac{\\partial y_{1}}{\\partial x_{n}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{n}} \\end{pmatrix} \\begin{pmatrix} \\frac{\\partial l}{\\partial y_{1}}\\\\ \\vdots \\\\ \\frac{\\partial l}{\\partial y_{m}} \\end{pmatrix} = \\begin{pmatrix} \\frac{\\partial l}{\\partial x_{1}}\\\\ \\vdots \\\\ \\frac{\\partial l}{\\partial x_{n}} \\end{pmatrix}$\n",
    "\n",
    "注意，$v^{T}\\cdot J$ 给出一个行向量，可以通过 $J^{T}\\cdot v$ 将其视为列向量\n",
    "\n",
    "vector-Jacobian product 使得外部梯度返回到具有非标量输出的模型变得非常方便。\n",
    "\n",
    "现在来看一个vector-Jacobian product的例子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  812.7551, -1214.2366,  -864.7861], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3, requires_grad=True)\n",
    "y = x * 2\n",
    "while y.data.norm() < 1000:\n",
    "    y = y * 2\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`y`不再是个标量。\n",
    "`torch.autograd`无法直接计算完整雅可比行列，如果只想要vector-Jacobian product，只需将向量作为参数传入`backward`："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5.1200e+01, 5.1200e+02, 5.1200e-02])\n"
     ]
    }
   ],
   "source": [
    "gradients = torch.tensor([0.1, 1.0, 0.0001], dtype=torch.float)\n",
    "y.backward(gradients)\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果``.requires_grad=True``，又不希望计算autograd，可以将变量包裹在 ``with torch.no_grad()``中:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(x.requires_grad)\n",
    "print((x**2).requires_grad)\n",
    "with torch.no_grad():\n",
    "    print((x**2).requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**稍后阅读:**\n",
    "\n",
    " ``autograd`` 和 ``Function`` 的官方文档 https://pytorch.org/docs/autograd\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch for Deeplearning",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
